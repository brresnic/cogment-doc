<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="canonical" href="https://docs.cogment.ai/cogment/tutorial/7-dqn-player/">
  <link rel="shortcut icon" href="../../../img/favicon.ico">
  <title>Step 7: Add a player trained with Reinforcement Learning using DQN - Cogment</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../../css/theme.css" />
  <link rel="stylesheet" href="../../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Step 7: Add a player trained with Reinforcement Learning using DQN";
    var mkdocs_page_input_path = "cogment/tutorial/7-dqn-player.md";
    var mkdocs_page_url = "/cogment/tutorial/7-dqn-player/";
  </script>
  
  <script src="../../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../../.." class="icon icon-home"> Cogment</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <p class="caption"><span class="caption-text">Introduction</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../..">Overview</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../../introduction/installation/">Installation & Setup</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Support</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../../support/community-channels/">Community channels</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Concepts</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../../concepts/core-concepts/">Core Concepts</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../../concepts/glossary/">Glossary</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Cogment</span></p>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="#">Tutorial</a>
    <ul class="current">
                <li class="toctree-l2"><a class="reference internal" href="../introduction/">Introduction</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../1-bootstrap-and-data-structures/">Step 1: Bootstrap a Cogment app</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2-random-player/">Step 2: Implement actor and environment</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../3-rewards/">Step 3: Introduce rewards</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../4-heuristic-player/">Step 4: Create a heuristic player</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../5-human-player/">Step 5: Add a human player in the loop</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../6-web-client/">Step 6: Implement a web client for the human player</a>
                </li>
                <li class="toctree-l2 current"><a class="reference internal current" href="./">Step 7: Add a player trained with Reinforcement Learning using DQN</a>
    <ul class="current">
    <li class="toctree-l3"><a class="reference internal" href="#creating-an-actor-service">Creating an actor service</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#playing-against-the-heuristic-player">Playing against the heuristic player</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#implementing-the-deep-q-network">Implementing the Deep Q Network</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#random-exploration">Random exploration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#replay-buffer">Replay buffer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#training">Training!</a>
    </li>
    </ul>
                </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../cogment-api-guide/">Cogment API Guide</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="#">Cogment API Reference</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../cogment-api-reference/cogment-yaml/">cogment.yaml</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../cogment-api-reference/python/">python</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="#">Javascript/Typescript SDK</a>
    <ul>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/modules/">Modules</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/classes/actorsession/">Class: ActorSession</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/classes/cogmentservice/">Class: CogmentService</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/classes/trialcontroller/">Class: TrialController</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/enums/eventtype/">Enum: EventType</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogmentyaml/">Interface: CogmentYaml</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogmentyamlactor/">Interface: CogmentYamlActor</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogmentyamlactorclass/">Interface: CogmentYamlActorClass</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogmentyamldatalog/">Interface: CogmentYamlDatalog</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogmentyamltrialparameters/">Interface: CogmentYamlTrialParameters</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogsettings/">Interface: CogSettings</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/cogsettingsactorclass/">Interface: CogSettingsActorClass</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/createserviceoptions/">Interface: CreateServiceOptions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/event/">Interface: Event</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/reward/">Interface: Reward</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/sendmessageoptions/">Interface: SendMessageOptions</a>
                </li>
                <li class="toctree-l3"><a class="reference internal" href="../../cogment-api-reference/javascript/interfaces/trialactor/">Interface: TrialActor</a>
                </li>
    </ul>
                </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="#">Cogment Low Level API Guide</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../cogment-low-level-api-guide/overview/">Overview</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../cogment-low-level-api-guide/grpc/">gRPC API Reference</a>
                </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Deployment Guide</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../../deployment-guide/local-deployement/">Local Deployment</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../../license/">License</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../..">Cogment</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../..">Docs</a> &raquo;</li>
    
      
        
          <li>Tutorial &raquo;</li>
        
      
        
          <li>Cogment &raquo;</li>
        
      
    
    <li>Step 7: Add a player trained with Reinforcement Learning using DQN</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/cogment/cogment-doc/edit/master/docs/cogment/tutorial/7-dqn-player.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="step-7-add-a-web-client-for-the-human-player">Step 7: Add a web client for the human player</h1>
<blockquote>
<p>This part of the tutorial follows <a href="../5-human-player/">step 5</a> and <a href="./6-human-player.md">step 6</a>, make sure you've gone through either one of those before starting this one. Alternatively the completed step 5 can be retrieved from the <a href="https://github.com/cogment/cogment-tutorial-rps">tutorial's repository</a>.</p>
</blockquote>
<p>In this step of the tutorial, we will go over yet another actor implementation and this implementation will be learning from its experience. We will implement an RPS player using Reinforcement Learning (RL) and more precisely a <a href="https://arxiv.org/pdf/1312.5602.pdf">Deep Q Network</a>, one of the foundational algorithm of modern RL.</p>
<p>While we will explain some aspects of RL and DQN along the way, we won't go into all the details. Interested readers can refer to <a href="http://incompleteideas.net/book/the-book-2nd.html">"Reinforcement Learning: An Introduction" by Richard S. Sutton and Andrew G. Barto</a> or to the original Deep Q Network article linked above.</p>
<h2 id="creating-an-actor-service">Creating an actor service</h2>
<p>Back in <a href="../4-heuristic-player">step 4</a>, we created a new implementation of the <code>player</code> actor class in the same service as the previous one. It was a sound choice for this implementation because it was small and didn't require additional dependencies. In some cases it makes more sense to create a fully separated service for a new actor implementation. This is what we will do here.</p>
<p>Start by copy/pasting the <code>random_agent</code> folder and name the copy <code>dqn_agent</code>. Let's then clean up <code>dqn_agent/main.py</code> to keep only a single actor implentation and name it <code>dqn_agent</code>. You should have something like the following.</p>
<pre><code class="python">import cog_settings
from data_pb2 import PlayerAction

import cogment

import asyncio
import random

async def dqn_agent(actor_session):
    # ...

async def main():
    print(&quot;Deep Q Network agents service up and running.&quot;)

    context = cogment.Context(cog_settings=cog_settings, user_id=&quot;rps&quot;)
    context.register_actor(
        impl=dqn_agent,
        impl_name=&quot;dqn_agent&quot;,
        actor_classes=[
            &quot;player&quot;,
        ],
    )

    await context.serve_all_registered(cogment.ServedEndpoint(port=9000))


if __name__ == &quot;__main__&quot;:
    asyncio.run(main())
</code></pre>

<p>Since we have created a new service we need to reference it at several places for everything to work properly. First, let's edit <code>docker-compose.yaml</code> to add the new service. To do that simply add the following under the <code>services</code> key, it tells docker-compose about the new service.</p>
<pre><code class="yaml">dqn-agent:
  build:
    context: dqn_agent
    dockerfile: ../py_service.dockerfile
</code></pre>

<p>Then we will need to edit <code>cogment.yaml</code> to make <code>cogment run generate</code> run in the new service's directory and have <code>cogment run build</code> and <code>cogment run start</code> respectively trigger its build and its start. We will change the <code>generate</code>, <code>build</code> and <code>start</code> key under <code>commands</code>.</p>
<pre><code class="yaml">commands:
  generate: &gt;
    cogment generate
    --python_dir environment
    --python_dir client
    --python_dir random_agent
    --python_dir dqn_agent
  # ...
  build: docker-compose build client dashboard metrics orchestrator environment random-agent dqn-agent
  # ...
  start: docker-compose up dashboard metrics orchestrator environment random-agent dqn-agent
</code></pre>

<p>Finally, the metrics server needs to know about this new data source. In <code>metrics/prometheus.yml</code>, add a new item under the <code>scrape_configs</code> key.</p>
<pre><code class="yaml">- job_name: &quot;dqn-agent&quot;
  dns_sd_configs:
    - names:
        - &quot;dqn-agent&quot;
      type: &quot;A&quot;
      port: 8000
      refresh_interval: 5s
</code></pre>

<h2 id="playing-against-the-heuristic-player">Playing against the heuristic player</h2>
<p>We will train our new player against the <a href="../4-heuristic-player/">heuristic player</a> we previously developed. We first need to update the trial config in <code>cogment.yaml</code>: <code>player_1</code> will be our new actor implementation while <code>player_2</code> will be the heuristic implementation, trials will be 20 games long to generate enough meaningful data between each training steps.</p>
<pre><code class="yaml">trial_params:
  environment:
    endpoint: grpc://environment:9000
    config:
      target_game_score: 2
      target_games_count: 20
  actors:
    - name: player_1
      actor_class: player
      implementation: dqn_agent
      endpoint: grpc://dqn-agent:9000
    - name: player_2
      actor_class: player
      implementation: heuristic_agent
      endpoint: grpc://random-agent:9000
</code></pre>

<p>We can also update <code>client/main.py</code> to run a bunch of trials sequentially.</p>
<pre><code class="py">async def main():
    print(&quot;Client starting...&quot;)

    context = cogment.Context(cog_settings=cog_settings, user_id=&quot;rps&quot;)

    # Create a controller
    controller = context.get_controller(endpoint=cogment.Endpoint(&quot;orchestrator:9000&quot;))

    # Start a trial campaign
    for i in range(1000):
        trial_id = await controller.start_trial(trial_config=TrialConfig())
        print(f&quot;Running trial #{i+1} with id '{trial_id}'&quot;)

        # Wait for the trial to end by itself
        async for trial_info in controller.watch_trials(
            trial_state_filters=[cogment.TrialState.ENDED]
        ):
            if trial_info.trial_id == trial_id:
                break
</code></pre>

<p>You can now <a href="../1-bootstrap-and-data-structures/#building-and-running-the-app">build and run</a> the application. It should take a few minutes to run as it goes through the trial campaign.</p>
<h2 id="implementing-the-deep-q-network">Implementing the Deep Q Network</h2>
<p>We have set everything up, we can now focus on implementing our DQN agent.</p>
<p>A Deep Q Network is a neural network taking an observation as input, and outputs the Q value for each of the actions in the action space. The Q Value is an estimation of the expected value of all the rewards if a given action is taken. The DQN agent action policy is therefore to take the action having the largest predicted Q Value. Let's start by implementing this part and we will then deal with training this model.</p>
<p>In the rest of this tutorial we will use <a href="https://www.tensorflow.org">Tensorflow and its Keras API</a> for the model itself, as well as <a href="https://numpy.org">numpy</a> for datastructures. Let's import these at the top of <code>dqn_agent/main.py</code>.</p>
<pre><code class="python">import numpy as np
import tensorflow as tf
</code></pre>

<p>Let's get into the meat of the matter by implementing a function to create our model. We are using <a href="https://www.tensorflow.org/guide/keras/functional">Keras functional API</a> to create the following layers:</p>
<ol>
<li>Two scalar inputs, the last moves of the player and the opponent.</li>
<li>Each input is <a href="https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics">one-hot encoded</a> to avoid assuming an unwanted ordering and quantitative relationship between the moves.</li>
<li>The two encoded inputs are concatenated to a single vector.</li>
<li>A dense non-linear hidden layer is added.</li>
<li>The output layers estimates the Q value for each move.</li>
</ol>
<p>Everything then gets wrapped up and returned.</p>
<p>This function is then used to create a global <code>_model</code> that we will use in the actor implementation.</p>
<pre><code class="python">MOVES = [ROCK, PAPER, SCISSORS]
actions_count = len(MOVES)

def create_model():
    # 1. Input layers
    in_me_last_move = tf.keras.Input(name=&quot;obs_me_last_move&quot;, shape=(1))
    in_them_last_move = tf.keras.Input(name=&quot;obs_them_last_move&quot;, shape=(1))
    # 2. One hot encoding of the layers
    one_hot_move = tf.keras.layers.experimental.preprocessing.CategoryEncoding(
        name=&quot;one_hot_move&quot;, max_tokens=len(MOVES), output_mode=&quot;binary&quot;
    )
    one_hot_me_last_move = one_hot_move(in_me_last_move)
    one_hot_them_last_move = one_hot_move(in_them_last_move)
    # 3. Concatenating the two inputs
    concat_ins = tf.keras.layers.concatenate(
        [one_hot_me_last_move, one_hot_them_last_move]
    )
    # 4. Dense hidden layer
    hidden_layer = tf.keras.layers.Dense(24, activation=&quot;relu&quot;)(concat_ins)
    # 5. Output
    outs = tf.keras.layers.Dense(actions_count, activation=&quot;linear&quot;)(hidden_layer)
    return tf.keras.Model(
        inputs=[in_me_last_move, in_them_last_move], outputs=outs, name=&quot;rps_dqn_policy&quot;
    )

_model = create_model()
</code></pre>

<p>The other piece of the puzzle is implementing a small function that'll convert our observations into inputs for the model we just created. As most of the encoding is handled by the model itself it's fairly straightforward.</p>
<pre><code class="python">def model_ins_from_observations(observations):
    return {
        &quot;obs_me_last_move&quot;: np.array([[o.snapshot.me.last_move] for o in observations]),
        &quot;obs_them_last_move&quot;: np.array(
            [[o.snapshot.them.last_move] for o in observations]
        ),
    }
</code></pre>

<p>Finally we can make it work together by replacing the random choice of action by the use of the model. At the moment the model will just use the random initialization weights so don't expect much !</p>
<p>Here is how the event loop in the <code>dqn_agent</code> function will need to be updated to:</p>
<ol>
<li>Use <code>model_ins_from_observations</code> to compute the model inputs,</li>
<li>Use the model in inference mode to compute the q value of each of the possible actions,</li>
<li>Finally do the action having the largest q value.</li>
</ol>
<pre><code class="python">if event.observation:
  model_ins = model_ins_from_observations([event.observation])
  if event.type == cogment.EventType.ACTIVE:
    model_outs = _model(model_ins, training=False)
    action = tf.math.argmax(model_outs[0]).numpy()
    actor_session.do_action(PlayerAction(move=action))
</code></pre>

<p>You can now <a href="../1-bootstrap-and-data-structures/#building-and-running-the-app">build and run</a> the application. It should take a few minutes to run as it goes through the trial campaign.</p>
<blockquote>
<p>In this example we define <code>_model</code> (and other variables in the following sections) as global mutable variables. It works in our case because the dqn agents are neither distributed nor multithreaded.</p>
</blockquote>
<h2 id="random-exploration">Random exploration</h2>
<p>With the previous code, you might have noticed that the agent will play exactly the same action given the same set of observations, this is because the weights of the model are fixed. However, especially at the beginning of the training process we want the agent to <em>experience</em> a variety of situations. We address this issue by introducing a decaying exploration rate <em>epsilon</em>.</p>
<p>First we will define as global variables, the parameters for this epsilon value: its minimum value, its maximum and initial value and its decay per tick. We also define as a global variable the current value of epsilon. You can add the following after the imports in <code>dqn_agent/main.py</code>.</p>
<pre><code class="python">epsilon_min = 0.05
epsilon_max = 1.0
epsilon_decay_per_tick = (
  epsilon_max - epsilon_min
) / 1000.0  # Linearly reach the lowest exploration rate after 1000 ticks

_epsilon = epsilon_max
</code></pre>

<p>We then create a simple function we can use everytime an action needs to be taken to retrieve and update <code>_epsilon</code></p>
<pre><code class="python">def get_and_update_epsilon():
  global _epsilon
  current_epsilon = _epsilon
  _epsilon -= epsilon_decay_per_tick
  _epsilon = max(_epsilon, epsilon_min)
  return current_epsilon
</code></pre>

<p>This function can then be used to do random actions occasionally to facilitate the exploration. To do that, we need to modify slightly how the actions are computed and submitted.</p>
<pre><code class="python">if event.type == cogment.EventType.ACTIVE:
  if np.random.rand(1)[0] &lt; get_and_update_epsilon():
    # Take random action
    action = np.random.choice(actions_count)
  else:
    model_outs = _model(model_ins, training=False)
    action = tf.math.argmax(model_outs[0]).numpy()
  actor_session.do_action(PlayerAction(move=action))
</code></pre>

<p>You can now <a href="../1-bootstrap-and-data-structures/#building-and-running-the-app">build and run</a> the application. Nothing should appear different at this stage.</p>
<h2 id="replay-buffer">Replay buffer</h2>
<p>In our journey to train a model, the next stage is to build an experience replay buffer to collect actions/observations/rewards triples over the course of the trials. Once done, it'll be usable to train the model using this data.</p>
<p>We will start by creating the datastructure. We are using a column-oriented structure relying on <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html">numpy arrays</a> as they interoperate easily with tensorflow and support the needed manipulation primitives. Each row is a <strong>sample</strong> corresponding to one tick: the received observation and reward, the selected action as well as the next tick's received observation.</p>
<pre><code class="python">def create_replay_buffer():
  return {
    &quot;obs_me_last_move&quot;: np.array([]),
    &quot;obs_them_last_move&quot;: np.array([]),
    &quot;action&quot;: np.array([]),
    &quot;reward&quot;: np.array([]),
    &quot;next_obs_me_last_move&quot;: np.array([]),
    &quot;next_obs_them_last_move&quot;: np.array([]),
  }

_rb = create_replay_buffer()
</code></pre>

<p>During each trial the agent will collect its data points in a <em>trial</em> replay buffer then append it to the global one. To achieve that we will first create the function in charge of the appending then collect data during the trial and call the "append" function.</p>
<p>The following function will take a <em>trial</em> replay buffer and append it to the global <code>_rb</code>. To avoid memory overflow the replay buffer size is capped.</p>
<pre><code class="python">_collected_samples_count = 0
max_replay_buffer_size = 100000

def append_trial_replay_buffer(trial_rb):
  global _rb
  global _collected_samples_count

  trial_rb_size = len(trial_rb[&quot;obs_me_last_move&quot;])

  for key in _rb.keys():
    # Append the trial data to the current vector
    _rb[key] = np.append(_rb[key], trial_rb[key])
    # Enfore the size limit by discarding older data
    if len(_rb[key]) &gt; max_replay_buffer_size:
        _rb[key] = _rb[key][-max_replay_buffer_size:]

  _collected_samples_count += trial_rb_size
  rb_size = len(_rb[&quot;obs_me_last_move&quot;])

  # Sanity check, all vectors in the replay buffer should have the same size
  for key in _rb.keys():
    assert rb_size == len(_rb[key])

  print(
    f&quot;{trial_rb_size} new samples stored after a trial, now having {rb_size} samples over a total of {_collected_samples_count} collected samples.&quot;
  )
</code></pre>

<p>The <code>dqn_agent</code> function can then be updated to collect received observations and reward and sent actions. By default every action gets a <em>zero</em> reward. When a reward for a specific tick is received its value gets updated.</p>
<pre><code class="python">async def dqn_agent(actor_session):
  actor_session.start()

  trial_rb = create_replay_buffer()

  async for event in actor_session.event_loop():
    if event.observation:
      model_ins = model_ins_from_observations([event.observation])
      if event.type == cogment.EventType.ACTIVE:
        # [...]
        trial_rb[&quot;obs_me_last_move&quot;] = np.append(
            trial_rb[&quot;obs_me_last_move&quot;], model_ins[&quot;obs_me_last_move&quot;]
        )
        trial_rb[&quot;obs_them_last_move&quot;] = np.append(
            trial_rb[&quot;obs_them_last_move&quot;], model_ins[&quot;obs_them_last_move&quot;]
        )
        trial_rb[&quot;action&quot;] = np.append(trial_rb[&quot;action&quot;], [action])
        trial_rb[&quot;reward&quot;] = np.append(trial_rb[&quot;reward&quot;], [0.0])
      else:
        trial_rb[&quot;obs_me_last_move&quot;] = np.append(
            trial_rb[&quot;obs_me_last_move&quot;], model_ins[&quot;obs_me_last_move&quot;]
        )
        trial_rb[&quot;obs_them_last_move&quot;] = np.append(
            trial_rb[&quot;obs_them_last_move&quot;], model_ins[&quot;obs_them_last_move&quot;]
        )
    for reward in event.rewards:
      trial_rb[&quot;reward&quot;][reward.tick_id] = reward.value

  # Shifting the observations to get the next observations
  trial_rb[&quot;next_obs_me_last_move&quot;] = trial_rb[&quot;obs_me_last_move&quot;][1:]
  trial_rb[&quot;next_obs_them_last_move&quot;] = trial_rb[&quot;obs_them_last_move&quot;][1:]
  # Dropping the last row, as it only contains the last observations
  trial_rb[&quot;obs_me_last_move&quot;] = trial_rb[&quot;obs_me_last_move&quot;][:-1]
  trial_rb[&quot;obs_them_last_move&quot;] = trial_rb[&quot;obs_them_last_move&quot;][:-1]
  append_trial_replay_buffer(trial_rb)
</code></pre>

<p>You can now <a href="../1-bootstrap-and-data-structures/#building-and-running-the-app">build and run</a> the application. The behavior should be the same but the log should confirm that data gets accumulated.</p>
<h2 id="training">Training!</h2>
<p>Here we are, all the pieces are in place we can implement the training proper. The function is a standard implementation of DQN and is decomposed in 4 steps:</p>
<ol>
<li>Select a random batch of samples from the replay buffer</li>
<li>Compute the target Q value for each sample from the received reward and the next observation using a previous version of the model.</li>
<li>(Re)compute the estimated Q value of each sample from the selected action and observation using the current version of the model.</li>
<li>Perform an optimization step of the model parameters trying to reduce the loss between the samples estimated and target q values.</li>
</ol>
<pre><code class="python">batch_size = 50  # Size of batch taken from replay buffer
gamma = 0.99  # Discount factor for future rewards
optimizer = tf.keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)
loss_function = tf.keras.losses.Huber()
target_model_update_interval = 1000

_target_model = create_model()

def train():
  global _model
  global _target_model

  rb_size = len(_rb[&quot;obs_me_last_move&quot;])

  if rb_size &gt;= batch_size:
    # Step 1 - Randomly select a batch
    batch_indices = np.random.choice(range(rb_size), size=batch_size)
    batch_rb = create_replay_buffer()
    for key in batch_rb.keys():
        batch_rb[key] = np.take(_rb[key], batch_indices)

    # Step 2 - Compute target q values
    ## Predict the expected reward for the next observation of each sample
    ## Use the target model for stability
    target_actions_q_values = _target_model(
      {
        &quot;obs_me_last_move&quot;: batch_rb[&quot;next_obs_me_last_move&quot;],
        &quot;obs_them_last_move&quot;: batch_rb[&quot;next_obs_them_last_move&quot;],
      }
    )

    ## target Q value = reward + discount factor * expected future reward
    target_q_values = batch_rb[&quot;reward&quot;] + gamma * tf.reduce_max(
      target_actions_q_values, axis=1
    )

    # Step 3 - Compute estimated q values
    ## Create masks of the taken actions to later select relevant q values
    selected_actions_masks = tf.one_hot(batch_rb[&quot;action&quot;], actions_count)

    with tf.GradientTape() as tape:
      ## Recompute q values for all the actions at each sample
      estimated_actions_q_values = _model(
        {
          &quot;obs_me_last_move&quot;: batch_rb[&quot;obs_me_last_move&quot;],
          &quot;obs_them_last_move&quot;: batch_rb[&quot;obs_them_last_move&quot;],
        }
      )

      ## Apply the masks to get the Q value for taken actions
      estimated_q_values = tf.reduce_sum(
        tf.multiply(estimated_actions_q_values, selected_actions_masks), axis=1
      )

      ## Compute loss between the target Q values and the estimated Q values
      loss = loss_function(target_q_values, estimated_q_values)
      print(f&quot;loss={loss.numpy()}&quot;)

      ## Backpropagation!
      grads = tape.gradient(loss, _model.trainable_variables)
      optimizer.apply_gradients(zip(grads, _model.trainable_variables))

    # Update the target model
    if _collected_samples_count % target_model_update_interval == 0:
      _target_model.set_weights(_model.get_weights())
</code></pre>

<p>This function then needs to be called at the end of each trial after the call to <code>append_trial_replay_buffer</code>.</p>
<p>You can now <a href="../1-bootstrap-and-data-structures/#building-and-running-the-app">build and run</a> the application. The dqn agent will start to learn and quickly prevails against the heuristic implementation.</p>
<p>This can be observed by opening the dashboard at <a href="http://localhost:3003">http://localhost:3003</a> and opening the reward page. You should be able to track the progression of the dqn implementation.</p>
<p><img alt="Cumulative reward by agent type diagram showing the dqn implementation prevailing against the heuristic agent" src="../figures/dqn_agent_rewards.png" /></p>
<p>This concludes the step 7 of the tutorial: you implemented your first trained actor implementation!</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../cogment-api-guide/" class="btn btn-neutral float-right" title="Cogment API Guide">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../6-web-client/" class="btn btn-neutral" title="Step 6: Implement a web client for the human player"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/cogment/cogment-doc/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../6-web-client/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../cogment-api-guide/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../../..';</script>
    <script src="../../../js/theme.js" defer></script>
      <script src="../../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
