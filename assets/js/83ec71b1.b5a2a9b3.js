"use strict";(self.webpackChunkcogment_doc=self.webpackChunkcogment_doc||[]).push([[240],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return u}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),d=p(n),u=r,h=d["".concat(l,".").concat(u)]||d[u]||m[u]||o;return n?a.createElement(h,i(i({ref:t},c),{},{components:n})):a.createElement(h,i({ref:t},c))}));function u(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var o=n.length,i=new Array(o);i[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var p=2;p<o;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3985:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return l},metadata:function(){return p},toc:function(){return c},default:function(){return d}});var a=n(7462),r=n(3366),o=(n(7294),n(3905)),i=["components"],s={},l="Step 3: Rewards",p={unversionedId:"cogment/tutorial/rewards",id:"cogment/tutorial/rewards",title:"Step 3: Rewards",description:"This part of the tutorial follows step 2, make sure you've gone through it before starting this one. Alternatively the completed step 2 can be retrieved from the tutorial's repository.",source:"@site/docs/cogment/tutorial/3-rewards.md",sourceDirName:"cogment/tutorial",slug:"/cogment/tutorial/rewards",permalink:"/docs/cogment/tutorial/rewards",tags:[],version:"current",lastUpdatedAt:1645720139,formattedLastUpdatedAt:"2/24/2022",sidebarPosition:3,frontMatter:{},sidebar:"docSidebar",previous:{title:"Step 2: Implement a first actor and environment",permalink:"/docs/cogment/tutorial/random-player"},next:{title:"Step 4: Add a second actor implementation based on a heuristic",permalink:"/docs/cogment/tutorial/heuristic-player"}},c=[{value:"Adding the concept of a game",id:"adding-the-concept-of-a-game",children:[],level:2},{value:"Sending rewards to the actors",id:"sending-rewards-to-the-actors",children:[],level:2},{value:"Using the <strong>metrics</strong> and <strong>dashboard</strong> services to retrieve &amp; visualize the rewards over time",id:"using-the-metrics-and-dashboard-services-to-retrieve--visualize-the-rewards-over-time",children:[],level:2}],m={toc:c};function d(e){var t=e.components,s=(0,r.Z)(e,i);return(0,o.kt)("wrapper",(0,a.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"step-3-rewards"},"Step 3: Rewards"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"This part of the tutorial follows ",(0,o.kt)("a",{parentName:"p",href:"/docs/cogment/tutorial/random-player"},"step 2"),", make sure you've gone through it before starting this one. Alternatively the completed step 2 can be retrieved from the ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/cogment/cogment-tutorial-rps"},"tutorial's repository"),".")),(0,o.kt)("p",null,"In this step of the tutorial, we will start thinking about rewards. Rewards are a way to evaluate how an actor performs at a task. They can be used to evaluate or compare different implementations of an actor, or, especially in the context or Reinforcement Learning, train an model. In Cogment, both the environment and other actors can evaluate an actor. Here, we will focus on sending rewards from the environment."),(0,o.kt)("p",null,"The first thing we'll do for this step is to add the concept of multi-round games to our RPS implementation. We'll learn to configure the environment along the way. Then, we will adapt the environment implementation to send a reward to the actor winning a game. Finally, we will retrieve rewards and other metrics from the running Cogment app."),(0,o.kt)("h2",{id:"adding-the-concept-of-a-game"},"Adding the concept of a game"),(0,o.kt)("p",null,"Up until now, our implementation of RPS focused on rounds. However, RPS is usually played in games won by the player reaching a target score, i.e. a number of won rounds."),(0,o.kt)("p",null,"Before sending rewards we need to adapt our implementation to support games. We will make the target score of each game configurable."),(0,o.kt)("p",null,"The generated data structure ",(0,o.kt)("inlineCode",{parentName:"p"},"EnvConfig"),", referenced within ",(0,o.kt)("inlineCode",{parentName:"p"},"cogment.yaml")," in ",(0,o.kt)("inlineCode",{parentName:"p"},"environment.config_type"),", defines the configuration of the environment. Let's add a ",(0,o.kt)("inlineCode",{parentName:"p"},"target_game_score")," numerical property to it."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-proto"},"message EnvConfig {\n  int32 target_game_score = 1;\n}\n")),(0,o.kt)("p",null,"Modify the ",(0,o.kt)("inlineCode",{parentName:"p"},"data.proto")," file with this update."),(0,o.kt)("p",null,"The environment implementation can now be updated to know about games."),(0,o.kt)("p",null,"During the ",(0,o.kt)("strong",{parentName:"p"},"initialization")," phase of the ",(0,o.kt)("inlineCode",{parentName:"p"},"environment")," function, we can retrieve the value from the environment's configuration."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"target_game_score = environment_session.config.target_game_score\n")),(0,o.kt)("p",null,"Instead of counting ",(0,o.kt)("em",{parentName:"p"},"rounds")," we update the ",(0,o.kt)("inlineCode",{parentName:"p"},"state")," variable to count ",(0,o.kt)("em",{parentName:"p"},"games")," and the score of the ongoing game."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'state = {\n    "games_count": 0,\n    "p1": {\n        "won_games_count": 0,\n        "current_game_score": 0\n    },\n    "p2": {\n        "won_games_count": 0,\n        "current_game_score": 0\n    },\n}\n')),(0,o.kt)("p",null,"In the ",(0,o.kt)("strong",{parentName:"p"},"event loop")," we need to make two changes."),(0,o.kt)("p",null,"First, Instead of counting the rounds, we will update each player ",(0,o.kt)("inlineCode",{parentName:"p"},"current_game_score"),"."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'if p1_state.won_last:\n    state["p1"]["current_game_score"] += 1\nelif p2_state.won_last:\n    state["p2"]["current_game_score"] += 1\n')),(0,o.kt)("p",null,"Second, once the observation is sent, we detect the end of each game and update the ",(0,o.kt)("inlineCode",{parentName:"p"},"state")," accordingly."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},'# Update the game scores\nif state["p1"]["current_game_score"] >= target_game_score:\n    state["games_count"] += 1\n    state["p1"]["current_game_score"] = 0\n    state["p2"]["current_game_score"] = 0\n    state["p1"]["won_games_count"] += 1\n\n    print(f"{p1.actor_name} won game #{state[\'games_count\']}")\nelif state["p2"]["current_game_score"] >= target_game_score:\n    state["games_count"] += 1\n    state["p1"]["current_game_score"] = 0\n    state["p2"]["current_game_score"] = 0\n    state["p2"]["won_games_count"] += 1\n\n    print(f"{p2.actor_name} won game #{state[\'games_count\']}")\n')),(0,o.kt)("p",null,"Finally, during the ",(0,o.kt)("strong",{parentName:"p"},"termination"),", we display stats about the games."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"print(f\"\\t * {state['games_count']} games played\")\nprint(f\"\\t * {p1.actor_name} won {state['p1']['won_games_count']} games\")\nprint(f\"\\t * {p2.actor_name} won {state['p2']['won_games_count']} games\")\n")),(0,o.kt)("p",null,"Modify the ",(0,o.kt)("inlineCode",{parentName:"p"},"environment/main.py")," file with these updates."),(0,o.kt)("p",null,"Now that the data structure is modified and the environment implementation uses it, we can define, for the default trial, a value for the ",(0,o.kt)("inlineCode",{parentName:"p"},"target_game_score")," property. Let's start with games of 2 winning rounds."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"trial_params:\n    environment:\n        endpoint: grpc://environment:9000\n        config:\n            target_game_score: 2\n")),(0,o.kt)("p",null,"Modify the ",(0,o.kt)("inlineCode",{parentName:"p"},"cogment.yaml")," file with this update."),(0,o.kt)("p",null,"You can now ",(0,o.kt)("a",{parentName:"p",href:"/docs/cogment/tutorial/bootstrap-and-data-structures#building-and-running-the-app"},"build and run")," the application to check that it works as expected."),(0,o.kt)("p",null,"In this simple implementation, the concept of game is local to the environment. It has no impact on the observation and action spaces, and thus no impact on the actor implementation. This means an actor wouldn't ",(0,o.kt)("em",{parentName:"p"},"know")," that the round it currently plays is the tie breaker in a game or its very first round. As a result the actor will play every round the same way."),(0,o.kt)("h2",{id:"sending-rewards-to-the-actors"},"Sending rewards to the actors"),(0,o.kt)("p",null,"The environment is now able to:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"compute when an actor wins a game,"),(0,o.kt)("li",{parentName:"ul"},"communicate this information to it and to the other Cogment app services,"),(0,o.kt)("li",{parentName:"ul"},"send ",(0,o.kt)("strong",{parentName:"li"},"rewards")," when an actor reaches a measurable goal, in our case, when it wins a game.")),(0,o.kt)("p",null,"Please note, that not all actions need to be rewarded."),(0,o.kt)("p",null,"When a game is won, the environment will add a ",(0,o.kt)("strong",{parentName:"p"},"positive reward to the winner")," (we chose a value of 1) and a ",(0,o.kt)("strong",{parentName:"p"},"negative reward to the loser")," (we chose a value of -1). Cogment also supports the notion of ",(0,o.kt)("em",{parentName:"p"},"confidence"),", a weight between 0 and 1 that expresses the qualification of the reward sender in its appreciation. In this case we are applying objective rules, so we use a confidence of 1."),(0,o.kt)("p",null,"In the ",(0,o.kt)("strong",{parentName:"p"},"event loop"),", when the first player wins a game we add the following."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"environment_session.add_reward(value=1, confidence=1, to=[p1.actor_name])\nenvironment_session.add_reward(value=-1, confidence=1, to=[p2.actor_name])\n")),(0,o.kt)("p",null,"When the second player wins a game we add the following."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-python"},"environment_session.add_reward(value=-1, confidence=1, to=[p1.actor_name])\nenvironment_session.add_reward(value=1, confidence=1, to=[p2.actor_name])\n")),(0,o.kt)("p",null,"Modify the ",(0,o.kt)("inlineCode",{parentName:"p"},"environment/main.py")," file to include the above additions."),(0,o.kt)("p",null,"You can now ",(0,o.kt)("a",{parentName:"p",href:"/docs/cogment/tutorial/bootstrap-and-data-structures#building-and-running-the-app"},"build and run")," the application to check that it works as expected. In particular you should see logs relative to the reception of rewards on the actor side."),(0,o.kt)("h2",{id:"using-the-metrics-and-dashboard-services-to-retrieve--visualize-the-rewards-over-time"},"Using the ",(0,o.kt)("strong",{parentName:"h2"},"metrics")," and ",(0,o.kt)("strong",{parentName:"h2"},"dashboard")," services to retrieve & visualize the rewards over time"),(0,o.kt)("p",null,"Now that we have introduced rewards, it's a good time to take a look at Cogment's dashboard, a web app centralizing metrics from the various Cogment components. To learn more about this, refer to the ",(0,o.kt)("a",{parentName:"p",href:"/docs/cogment-components/dashboard"},"dedicated page"),"."),(0,o.kt)("p",null,"If it's not already running, start the Cogment app:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-console"},"$ cogment run start\n")),(0,o.kt)("p",null,"In your browser of choice you can now open ",(0,o.kt)("a",{parentName:"p",href:"http://localhost:3003"},"http://localhost:3003")," and explore. If you haven't yet launched any trials, it should be mostly empty. To launch a trial, proceed as usual: in another terminal run,"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-console"},"$ cogment run client\n")),(0,o.kt)("p",null,"You can now check the ",(0,o.kt)("strong",{parentName:"p"},"reward dashboard")," to see which of the players performs better. Keep in mind that both players play randomly so don't expect to observe any interesting trends yet. You can launch several trials to get more data."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"Reward dashboard with 2 random players",src:n(1485).Z,width:"1589",height:"1057"})),(0,o.kt)("p",null,"The screenshot above shows the default reward dashboard. On the top row, it shows the reward received per individual actor, on the left is the rate averaged over the last 2 minutes, on the right is the total per trial. On the middle row, you'll find the same metrics but aggregated per actor implementation, one in our case. On the bottom row is the averaged reward over the last 5 minutes for all the actors."),(0,o.kt)("p",null,"In ",(0,o.kt)("a",{parentName:"p",href:"/docs/cogment/tutorial/dqn-player"},"tutorial 7"),", we will use this dashboard to monitor the training of an agent implementation."),(0,o.kt)("p",null,"This concludes the step 3 of the tutorial: you've learned about environment configuration, implemented your reward sending and used the ",(0,o.kt)("strong",{parentName:"p"},"metrics")," and ",(0,o.kt)("strong",{parentName:"p"},"dashboard")," services."),(0,o.kt)("p",null,"Let\u2019s move on to implementing an RPS player that actually considers what was played before deciding on its next move in ",(0,o.kt)("a",{parentName:"p",href:"/docs/cogment/tutorial/heuristic-player"},"step 4"),"."))}d.isMDXComponent=!0},1485:function(e,t,n){t.Z=n.p+"assets/images/reward_dashboard-47da12764a37273327e4dd2b14e9922f.png"}}]);